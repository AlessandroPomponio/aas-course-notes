@article{10.1093/mind/LIX.236.433,
    author = {Turing, A. M.},
    title = "{I.—Computing Machinery and Intelligence}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}

@article{4066245,
    author={M. {Minsky}},
    title={Steps toward Artificial Intelligence},
    journal={Proceedings of the IRE},
    volume={49},
    number={1},
    pages={8-30},
    year={1961},
    doi={10.1109/JRPROC.1961.287775}
}

@article{Watkins1992,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  added-at = {2020-01-01T20:16:30.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2416ac9f845c6ccea5a7eacee4dedead8/lanteunis},
  day = 01,
  doi = {10.1007/BF00992698},
  interhash = {a4436f9e14335d677f156049cb798253},
  intrahash = {416ac9f845c6ccea5a7eacee4dedead8},
  issn = {1573-0565},
  journal = {Machine Learning},
  keywords = {DRLAlgoComparison q-learning reinforcement_learning},
  month = may,
  number = 3,
  pages = {279--292},
  timestamp = {2020-01-01T20:16:30.000+0100},
  title = {Q-learning},
  url = {https://doi.org/10.1007/BF00992698},
  volume = 8,
  year = 1992
}

@article{MCCULLOCH199099,
title = {A logical calculus of the ideas immanent in nervous activity},
journal = {Bulletin of Mathematical Biology},
volume = {52},
number = {1},
pages = {99-115},
year = {1990},
issn = {0092-8240},
doi = {https://doi.org/10.1016/S0092-8240(05)80006-0},
url = {https://www.sciencedirect.com/science/article/pii/S0092824005800060},
author = {Warren S. McCulloch and Walter Pitts},
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.}
}

@InProceedings{10.1007/978-3-642-70911-1_15,
author="Shaw, G. L.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Donald Hebb: The Organization of Behavior",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="231--233",
abstract="I consider this a great privilege to be able to briefly remark on D.O. Hebb's marvellous book ``Organization of Behavior: A Neuropsychological Theory'' which he wrote in 1949. Hebb's ideas have had a profound influence on brain theory, in particular his famous ``A Neurophysiological Postulate'' governing the correlated pre-post synaptic changes which are the basis for the engram or memory trace. Although, there are many different forms of Hebb's postulate, I believe that essentially all ``viable'' mammalian cortical models embody some version of his idea: ``Let us assume then that the persistence or repetition of a reverberatory activity (or ``trace'') tends to induce lasting cellular changes that add to its stability. The assumption can be precisely stated as follows:When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A `s efficiency, as one of the cells firing B, is increased''.",
isbn="978-3-642-70911-1"
}

@inbook{10.5555/104279.104291,
author = {Hinton, G. E. and Sejnowski, T. J.},
title = {Learning and Relearning in Boltzmann Machines},
year = {1986},
isbn = {026268053X},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
pages = {282–317},
numpages = {36}
}

@article{10.1162/neco.2006.18.7.1527,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
title = {A Fast Learning Algorithm for Deep Belief Nets},
year = {2006},
issue_date = {July 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {18},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2006.18.7.1527},
doi = {10.1162/neco.2006.18.7.1527},
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
journal = {Neural Comput.},
month = jul,
pages = {1527–1554},
numpages = {28}
}

@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={G. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314}
}

@article{mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{10.1007/BF00992699,
author = {Lin, Long-Ji},
title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992699},
doi = {10.1007/BF00992699},
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks: adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
journal = {Mach. Learn.},
month = may,
pages = {293–321},
numpages = {29},
keywords = {connectionist networks, planning, teaching, Reinforcement learning}
}

@book{wooldridge02,
  abstract = {This is the first textbook to be explicitly designed
                  for use as a course text for an
                  undergraduate/graduate course on multi-agent
                  systems. Assuming only a basic understanding of
                  computer science, this text provides an introduction
                  to all the main issues in the theory and practice of
                  intelligent agents and multi-agent systems. The
                  companion Web Site includes sample exercises,
                  lecture slidest and hyperlinks to software referred
                  to in the book. Introduces agents, explains what
                  agents are, how they are constructed and how they
                  can be made to co-operate effectively with one
                  another in large-scale systems. Introduces the main
                  issues surrounding the design of intelligent
                  agents. Introduces a number of typical applications
                  for agent technology. },
  added-at = {2007-08-30T11:24:47.000+0200},
  author = {Wooldridge, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2fa6fda67ab7f81abee14443c05a631bc/vittorio.loreto},
  cluster = {2968919011107071265},
  googleid = {0qOxD1snnlMJ:scholar.google.com/},
  interhash = {c95510e600f16a7562e8ea01abab66fa},
  intrahash = {fa6fda67ab7f81abee14443c05a631bc},
  isbn = {047149691X},
  keywords = {2002 RMP_CFL agent-based multiagent woolbridge},
  publisher = {John Wiley and Sons},
  timestamp = {2007-08-30T11:24:47.000+0200},
  title = {Introduction to MultiAgent Systems},
  year = 2002
}

@InProceedings{10.1007/978-3-030-01713-2_1,
author="Tuyls, K.
and Stone, P.",
editor="Belardinelli, Francesco
and Argente, Estefan{\'i}a",
title="Multiagent Learning Paradigms",
booktitle="Multi-Agent Systems and Agreement Technologies",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="3--21",
abstract="``Perhaps a thing is simple if you can describe it fully in several different ways, without immediately knowing that you are describing the same thing'' -- Richard Feynman",
isbn="978-3-030-01713-2"
}

@book{ShohamLeytonBrown09,
  abstract = {Multiagent systems combine multiple autonomous entities, each having diverging interests or different information. This overview of the field offers a computer science perspective, but also draws on ideas from game theory, economics, operations research, logic, philosophy and linguistics. It will serve as a reference for researchers in each of these fields, and be used as a text for advanced undergraduate or graduate courses. The authors emphasize foundations to create a broad and rigorous treatment of their subject, with thorough presentations of distributed problem solving, game theory, multiagent communication and learning, social choice, mechanism design, auctions, cooperative game theory, and modal logics of knowledge and belief. For each topic, basic concepts are introduced, examples are given, proofs of key results are offered, and algorithmic considerations are examined. An appendix covers background material in probability theory, classical logic, Markov decision processes and mathematical programming.},
  added-at = {2017-01-02T18:13:22.000+0100},
  address = {Cambridge, UK},
  author = {Shoham, Yoav and Leyton-Brown, Kevin},
  biburl = {https://www.bibsonomy.org/bibtex/29e72e68e6763ab364dc88cd581b99ff3/flint63},
  file = {Preprint:2009/ShohamLeytonBrown09.pdf:PDF;Cambridge University Press Product Page:http\://www.cambridge.org/9780521899437:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0521899435/:URL;Related Web Site:http\://www.masfoundations.org/:URL},
  groups = {public},
  interhash = {16fd1c64a5808e6bfa8a6e6d38f6530c},
  intrahash = {9e72e68e6763ab364dc88cd581b99ff3},
  isbn = {978-0-521-89943-7},
  keywords = {01614 102 book ai agent middleware theory algorithm logic},
  publisher = {Cambridge University Press},
  timestamp = {2018-04-16T12:03:30.000+0200},
  title = {Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
  username = {flint63},
  year = 2009
}

@article{DBLP:journals/corr/TampuuMKKKAAV15,
  author    = {Ardi Tampuu and
               Tambet Matiisen and
               Dorian Kodelja and
               Ilya Kuzovkin and
               Kristjan Korjus and
               Juhan Aru and
               Jaan Aru and
               Raul Vicente},
  title     = {Multiagent Cooperation and Competition with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.08779},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08779},
  archivePrefix = {arXiv},
  eprint    = {1511.08779},
  timestamp = {Mon, 13 Aug 2018 16:46:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TampuuMKKKAAV15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}