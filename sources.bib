@article{10.1093/mind/LIX.236.433,
    author = {Turing, A. M.},
    title = "{I.—Computing Machinery and Intelligence}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}

@article{4066245,
    author={M. {Minsky}},
    title={Steps toward Artificial Intelligence},
    journal={Proceedings of the IRE},
    volume={49},
    number={1},
    pages={8-30},
    year={1961},
    doi={10.1109/JRPROC.1961.287775}
}

@article{Watkins1992,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  added-at = {2020-01-01T20:16:30.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2416ac9f845c6ccea5a7eacee4dedead8/lanteunis},
  day = 01,
  doi = {10.1007/BF00992698},
  interhash = {a4436f9e14335d677f156049cb798253},
  intrahash = {416ac9f845c6ccea5a7eacee4dedead8},
  issn = {1573-0565},
  journal = {Machine Learning},
  keywords = {DRLAlgoComparison q-learning reinforcement_learning},
  month = may,
  number = 3,
  pages = {279--292},
  timestamp = {2020-01-01T20:16:30.000+0100},
  title = {Q-learning},
  url = {https://doi.org/10.1007/BF00992698},
  volume = 8,
  year = 1992
}

@article{MCCULLOCH199099,
title = {A logical calculus of the ideas immanent in nervous activity},
journal = {Bulletin of Mathematical Biology},
volume = {52},
number = {1},
pages = {99-115},
year = {1990},
issn = {0092-8240},
doi = {https://doi.org/10.1016/S0092-8240(05)80006-0},
url = {https://www.sciencedirect.com/science/article/pii/S0092824005800060},
author = {Warren S. McCulloch and Walter Pitts},
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.}
}

@InProceedings{10.1007/978-3-642-70911-1_15,
author="Shaw, G. L.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Donald Hebb: The Organization of Behavior",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="231--233",
abstract="I consider this a great privilege to be able to briefly remark on D.O. Hebb's marvellous book ``Organization of Behavior: A Neuropsychological Theory'' which he wrote in 1949. Hebb's ideas have had a profound influence on brain theory, in particular his famous ``A Neurophysiological Postulate'' governing the correlated pre-post synaptic changes which are the basis for the engram or memory trace. Although, there are many different forms of Hebb's postulate, I believe that essentially all ``viable'' mammalian cortical models embody some version of his idea: ``Let us assume then that the persistence or repetition of a reverberatory activity (or ``trace'') tends to induce lasting cellular changes that add to its stability. The assumption can be precisely stated as follows:When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A `s efficiency, as one of the cells firing B, is increased''.",
isbn="978-3-642-70911-1"
}

@inbook{10.5555/104279.104291,
author = {Hinton, G. E. and Sejnowski, T. J.},
title = {Learning and Relearning in Boltzmann Machines},
year = {1986},
isbn = {026268053X},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
pages = {282–317},
numpages = {36}
}

@article{10.1162/neco.2006.18.7.1527,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
title = {A Fast Learning Algorithm for Deep Belief Nets},
year = {2006},
issue_date = {July 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {18},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2006.18.7.1527},
doi = {10.1162/neco.2006.18.7.1527},
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
journal = {Neural Comput.},
month = jul,
pages = {1527–1554},
numpages = {28}
}