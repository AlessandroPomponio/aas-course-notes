@article{10.1093/mind/LIX.236.433,
    author = {Turing, A. M.},
    title = "{I.—Computing Machinery and Intelligence}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}

@article{4066245,
    author={M. {Minsky}},
    title={Steps toward Artificial Intelligence},
    journal={Proceedings of the IRE},
    volume={49},
    number={1},
    pages={8-30},
    year={1961},
    doi={10.1109/JRPROC.1961.287775}
}

@article{Watkins1992,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  added-at = {2020-01-01T20:16:30.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2416ac9f845c6ccea5a7eacee4dedead8/lanteunis},
  day = 01,
  doi = {10.1007/BF00992698},
  interhash = {a4436f9e14335d677f156049cb798253},
  intrahash = {416ac9f845c6ccea5a7eacee4dedead8},
  issn = {1573-0565},
  journal = {Machine Learning},
  keywords = {DRLAlgoComparison q-learning reinforcement_learning},
  month = may,
  number = 3,
  pages = {279--292},
  timestamp = {2020-01-01T20:16:30.000+0100},
  title = {Q-learning},
  url = {https://doi.org/10.1007/BF00992698},
  volume = 8,
  year = 1992
}

@article{MCCULLOCH199099,
title = {A logical calculus of the ideas immanent in nervous activity},
journal = {Bulletin of Mathematical Biology},
volume = {52},
number = {1},
pages = {99-115},
year = {1990},
issn = {0092-8240},
doi = {https://doi.org/10.1016/S0092-8240(05)80006-0},
url = {https://www.sciencedirect.com/science/article/pii/S0092824005800060},
author = {Warren S. McCulloch and Walter Pitts},
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.}
}

@InProceedings{10.1007/978-3-642-70911-1_15,
author="Shaw, G. L.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Donald Hebb: The Organization of Behavior",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="231--233",
abstract="I consider this a great privilege to be able to briefly remark on D.O. Hebb's marvellous book ``Organization of Behavior: A Neuropsychological Theory'' which he wrote in 1949. Hebb's ideas have had a profound influence on brain theory, in particular his famous ``A Neurophysiological Postulate'' governing the correlated pre-post synaptic changes which are the basis for the engram or memory trace. Although, there are many different forms of Hebb's postulate, I believe that essentially all ``viable'' mammalian cortical models embody some version of his idea: ``Let us assume then that the persistence or repetition of a reverberatory activity (or ``trace'') tends to induce lasting cellular changes that add to its stability. The assumption can be precisely stated as follows:When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A `s efficiency, as one of the cells firing B, is increased''.",
isbn="978-3-642-70911-1"
}

@inbook{10.5555/104279.104291,
author = {Hinton, G. E. and Sejnowski, T. J.},
title = {Learning and Relearning in Boltzmann Machines},
year = {1986},
isbn = {026268053X},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
pages = {282–317},
numpages = {36}
}

@article{10.1162/neco.2006.18.7.1527,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
title = {A Fast Learning Algorithm for Deep Belief Nets},
year = {2006},
issue_date = {July 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {18},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2006.18.7.1527},
doi = {10.1162/neco.2006.18.7.1527},
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
journal = {Neural Comput.},
month = jul,
pages = {1527–1554},
numpages = {28}
}

@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={G. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314}
}

@article{mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{10.1007/BF00992699,
author = {Lin, Long-Ji},
title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992699},
doi = {10.1007/BF00992699},
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks: adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
journal = {Mach. Learn.},
month = may,
pages = {293–321},
numpages = {29},
keywords = {connectionist networks, planning, teaching, Reinforcement learning}
}