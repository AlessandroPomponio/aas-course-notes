\chapter{Multi-Armed Bandits}
In this chapter we want to study the evaluative and explorative aspects of reinforcement learning in a simplified setting that does not involve learning to act in more than one situation.

\section{The k-armed bandit problem}
To do so, we introduce the \textbf{$\boldsymbol{k}$-armed bandit problem} (a slot machine with $k$ levers), where we must choose between $k$ different options (the $k$ arms of the bandit) and after each choice we receive a reward taken from a \textbf{stationary} probability distribution depending on the action we selected. Our objective is to maximize the expected total reward over a certain number of time steps.

As a note, there is a different version of the $k$-armed bandit problem, known as \textbf{contextual bandits}, in which we do consider the state but assume that it does not depend on the previous actions. This problem is at the basis of ad-serving platforms.

Let us now see things from a more formal point of view: in our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the \textit{value} of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. Our goal is then to maximize the following:

\begin{gather*}
    \mathbb{E} \left[ G_t \  \middle\vert \  S_t = s, A_t = a \right] \doteq \mathbb{E} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \  \middle\vert \  S_t = s, A_t = a \right] \\
    = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \  \middle\vert \  S_t = s, A_t = a \right]
\end{gather*}

Since one of the assumptions of this problem is that the state does not depend on the actions taken, we can think of it as a constant $\bar{s}$ and substitute it in the formula above, obtaining:

\begin{gather*}
    \mathbb{E} \left[ G_t \  \middle\vert \  S_t = \bar{s}, A_t = a \right] \doteq \mathbb{E} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \  \middle\vert \  S_t = \bar{s}, A_t = a \right] \\
    = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \  \middle\vert \  S_t = \bar{s}, A_t = a \right]
\end{gather*}

The value of an arbitrary action a denoted as $q_*(a)$ is the expected reward given that $a$ is selected. More formally:

\begin{equation*}
    q_*(a) \doteq \mathbb{E} \left[ R_t \  \middle\vert \  A_t = a \right]
\end{equation*}

(Note how the expected reward does not include the state, as it is always the same). If we disregard the trivial case in which we already know the value of each action, we need to play to be able to estimate the value of the various actions: we denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$ and we want it to be as close as possible to $q_*(a)$.

If we maintain the estimates of each action value, then at any time step there will be at least one action whose value is the greatest (we refer to this action as the \textbf{greedy action}). When we select one of these actions, we are \textbf{exploiting our current knowledge} of the value of the actions. If instead we choose one of the nongreedy actions, then we are \textbf{exploring}, because this enables us to improve our estimate of the nongreedy actionâ€™s value. Exploitation maximizes the expected reward on the step, but by exploring, we may obtain a greater total reward in the long run (this is particularly true if our estimates have high uncertainty). In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps.

\section{Evaluating action-value methods}
Now that we have understood the need for estimating the values of actions and for using the estimates to make action selection decisions (which we collectively call \textbf{action-value methods}), we must look at some methods to do so.

In the case of the $k$-armed bandits problem, the value of an action is the mean reward when that action is selected, since we only have one state. A possible way to estimate this is by averaging the rewards we actually received:

\begin{gather}
    Q_t(a) \doteq \frac{\text{sum of rewards when an action } a \text{ is taken prior to time } t}{\text{number of times an action } a \text{ is taken prior to time } t}\nonumber\\
    = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbbm{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbbm{1}_{A_i = a}}
    \label{eq:ch3-sampleaveragemethod}
\end{gather}

Where $\mathbbm{1}_{\text{predicate}}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$  as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$. We call this the \textbf{sample-average method} for estimating action values because each estimate is an average of the sample of relevant rewards.

When it comes to selecting actions, the simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We can formalize this \textbf{greedy action selection method} as:

\begin{equation}
    A_t \doteq \argmax_a  Q_t(a)
    \label{eq:ch3-greedyactionselectionmethod}
\end{equation}

Where $\argmax_a$ denotes the action $a$ for which the expression that follows is maximized (with ties broken arbitrarily). The greedy action selection always exploits the current knowledge to maximize the immediate reward, disregarding exploration. To include it, we can add a small probability $\epsilon$ where the next action is selected randomly from all the actions with equal probability. This is called the \textbf{$\boldsymbol{\epsilon}$-greedy selection rule} and it has the advantage that in the limit, as the number of steps increases, every action will be sampled an infinite number of times, ensuring that all the $Q_t(a)$ converge to their respective $q_*(a)$ (these, however, are just asymptotic guarantees and say little about the practical effectiveness of these methods).

\section{Incremental implementation}
We now move to the question of how the methods that we have seen before can be computed in a computationally efficient manner, with constant memory and constant per-time-step computation.

To simplify the notation, we will concentrate on a single action $a$. Let $R_i(a)$ denote the reward received after the $i$-th selection of the action $a$, and let $Q_n$ denote the estimate of its action value after it has been selected $n-1$ times. We can now write:

\begin{equation*}
    Q_n(a) \doteq \frac{R_1(a) + R_2(a) + ... + R_{n-1}(a)}{n-1}
\end{equation*}

A trivial implementation of this would be to maintain a record of all the rewards and then perform this computation whenever the estimate value is needed. However, both memory and computation requirements would grow over time as more and more rewards are received. This can be avoided by considering the following steps:

\begin{equation*}
    \begin{split}
        Q_{n+1} & = \frac{1}{n} \sum_{i=1}^{n} R_i\\
        & = \frac{1}{n} \Big( R_n + \sum_{i=1}^{n-1} R_i \Big) \\
        & = \frac{1}{n} \Big( R_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \Big) \\
        & = \frac{1}{n} \Big( R_n + (n-1) Q_n \Big) \\
        & = \frac{1}{n} \Big( R_n + n Q_n - Q_n \Big) \\
        & = Q_n + \frac{1}{n} \Big[R_n - Q_n \Big]
    \end{split}
\end{equation*}

This type of update rule is quite common in reinforcement learning and follows this general formula:

\begin{equation}
    \text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize}(\text{Target} - \text{OldEstimate})
    \label{eq:ch3-generalrlupdaterule}
\end{equation}

The expression $(\text{Target} - \text{OldEstimate})$ is usually defined as the \textbf{error} in the estimate. It is reduced by taking a step towards the $\text{Target}$, which is presumed to indicate a desirable direction in which to move (though it may be noisy).

\section{Tracking a nonstationary problem}
The averaging method we just discussed is appropriate for stationary bandit problems, where the distribution of the rewards does not change over time. Many problems, however, fall in the nonstationary category and in those cases, it makes sense to give more weight to recent rewards, rather than long-gone ones. One of the most popular ways of dealing with these problems is using a \textbf{constant step-size parameter} $\boldsymbol{\alpha}$ (like $\frac{1}{n}$ in the formula above) in the range $]0,1]$ as such:

\begin{equation}
    Q_{n+1} \doteq Q_n + \alpha \left[R_n - Q_n \right]
    \label{eq:ch3-nonstationaryincrementalupdaterule}
\end{equation}

\textbf{This results in $\boldsymbol{Q_{n+1}}$ being a weighted average of past rewards and the initial estimate $Q_1$:}

\begin{equation}
    \begin{split}
        Q_{n+1} &= Q_n + \alpha \Big[ R_n - Q_n \Big] \\
        &= \alpha R_n + (1 - \alpha) Q_n \\
        &= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\
        &= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
        &= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + ... + (1 - \alpha)^{n-1} \alpha R_1 + (1 - \alpha)^n Q_1 \\
        &= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i
    \end{split}
    \label{eq:ch3-exponentialrecencyweightedaverage}
\end{equation}

This is called a weighted average since the sum of the weights $(1 - \alpha)^n + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} = 1$, as we can see here:

\begin{equation*}
    \begin{split}
        & (1 - \alpha)^n + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} \\
        &= (1 - \alpha)^n + \alpha (1 - \alpha)^n \sum_{i=1}^{n} (1 - \alpha)^{-i} \\
        &= (1 - \alpha)^n + \alpha (1 - \alpha)^n \Big(-1 + \sum_{i=0}^{n} (1 - \alpha)^{-i} \Big) \\
        &= (1 - \alpha)^{n+1} + \alpha (1 - \alpha)^n \sum_{i=0}^{n} (1 - \alpha)^{-i} \\
        &= (1 - \alpha)^{n+1} + \alpha (1 - \alpha)^n \frac{\alpha + (1 - \alpha)^{-n} -1}{\alpha} \\
        &= (1 - \alpha)^{n+1} + (1 - \alpha)^n \Big( \alpha + (1 - \alpha)^{-n} -1 \Big) \\
        &= (1 - \alpha)^{n+1} + \alpha (1 - \alpha)^n + 1 - (1 - \alpha)^n \\
        &= (1 - \alpha)^{n+1} + (1 - \alpha)^n (\alpha - 1) + 1 \\
        &= (1 - \alpha)^{n+1} - (1 - \alpha)^{n+1} + 1 = 1
    \end{split}
\end{equation*}

The quantity $1 - \alpha$ is less than $1$, and thus the weight given to $R_i$ decreases as the number of rewards increases. In fact, the weight decreases exponentially according to the exponent on $1 - \alpha$; accordingly, this is sometimes called an \textbf{exponential recency-weighted average}.

\section{Optimistic initial values}
All the methods that we have discussed until now depend to some extent on the initial action-value estimates (they are \textbf{biased} by their initial estimates). For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with a constant $\alpha$ the bias is permanent (although decreasing over time as given by the equation \eqref{eq:ch3-exponentialrecencyweightedaverage}). In practice, this kind of bias is usually not a problem and can sometimes be helpful, especially to supply some prior knowledge or to encourage exploration. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user.

Choosing a wildly optimistic initial value encourages action-value methods to explore: whichever actions are initially selected, the reward is less than the starting estimates; the learner will then switch to other actions, being ``disappointed'' with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge, making the system do a fair amount of exploration even with a fully greedy action selection strategy. Initially, the optimistic method performs worse because it explores more, but eventually it performs better as its exploration decreases with time. We call this technique for encouraging exploration \textbf{optimistic initial values}. It is a simple trick that can be quite effective on stationary problems, while it is not well suited to nonstationary problems, as its drive for exploration is inherently temporary (any method that focuses on the initial conditions is unlikely to help with the general nonstationary case).

\section{Upper-Confidence-Bound Action Selection}
Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. $\epsilon$-greedy action selection forces non-greedy (exploratory) actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to the following formula:

\begin{equation}
    A_t \doteq \argmax_a \Bigg[ Q_t (a) + c + \sqrt{\frac{\ln{t}}{N_t (a)}} \Bigg]
    \label{eq:ch3-upperconfidenceboundactionselection}
\end{equation}

Where $t$ is the time at which action $A_t$ is taken, $N_t (a)$ denotes the number of times that action a has been selected prior to time $t$ and the number $c > 0$ controls the degree of exploration. The idea of this \textbf{upper confidence bound (UCB)} action selection is that the square-root term is a measure of the uncertainty or variance of the estimate of the actual value of action $a$. The quantity being maxed over is thus a sort of upper bound on the possible true value of action $a$, with $c$ determining the confidence level. Each time $a$ is selected, the uncertainty is presumably reduced: $N_t (a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than $a$ is selected, $t$ increases but $N_t (a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.

\section{Contextual bandits}
Multi-armed bandits are used when the selection of the action does not depend on the state, but for many application (e.g., ads on a webpage), the state is very important. When the action selection depends on the state, but not on its previous history, we speak of \textbf{contextual bandits}. They are examples of \textbf{associative search tasks}, as they involve both trial-and-error learning to \textbf{search} for the best actions, and \textbf{association} of these actions with the situation in which they are best (e.g., if we are on a website about sportscars, we will be served ads about cars and not about flowers). Contextual bandits are still not a ``full'' reinforcement learning problem, as actions do not affect the future state, but only the immediate reward.