\chapter{Temporal Difference Learning}
Monte Carlo methods introduced us to ways of learning from raw experience without a model of the environmentâ€™s dynamics, iteratively generating episodes and analyzing their returns once they have been played out. Temporal Difference methods behave similarly, but overcome the limit of having to wait for the whole episode to end, enabling step-by-step learning. Let us analyze this in a bit more detail by first focusing on the \textit{prediction problem}.

Following the structure \eqref{eq:ch3-generalrlupdaterule} we saw in chapter 3, we can schematize a simple every-visit Monte Carlo method suitable for nonstationary environment as:

\begin{equation*}
    V(S_t) \leftarrow V(S_t) + \alpha \left[ G_t - V(S_t) \right]
\end{equation*}

Where $V(S_t)$ is the state-value function for state $S_t$, $\alpha$ is a constant step-size parameter and $G_t$ is the actual \textbf{return} following time $t$ (that we can only know at the end of the episode). Temporal Difference methods, on the contrary, only need to wait until the next time step: at time $t+1$ they immediately form a target and make a useful update using the observed \textbf{reward} $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest Temporal difference method makes the update:

\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
    \label{eq:ch5-genericonpolicytdupdate}
\end{equation}

Immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the Temporal Difference update is $R_{t+1} + \gamma V(S_{t+1})$. This Temporal Difference method is called \textbf{TD(0)}, or \textbf{one-step TD}, because it is a special case of the \textbf{TD(}$\boldsymbol{\lambda}$\textbf{)} and $n$-step TD methods that we will see later on.

\section{TD(0)}
An implementation of the TD(0) algorithm is shown in the box below:

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{the policy $\pi$ to be evaluated}
\Parameters{step size $\alpha \in \left] 0,1 \right]$}
\Initialize{
    $V(s),\ \forall s \in \mathcal{S}^+$, arbitrarily except that $V(terminal) = 0$
}

 \Loop{for each episode}{
    Initialize S\;
    \Loop{for each step of the episode}{
        $A \leftarrow$ action given by $\pi$ for $S$\;
        Take action $A$, observe $R, S'$\;
        $V(S) \leftarrow V(S) + \alpha \left[ R + \gamma V(S') - V(S) \right]$\;
        $S \leftarrow S'$\;
    }{until $S$ is terminal}
 }
\caption{Tabular TD(0) for estimating $v_\pi$}
\end{algorithm}

We can note that the quantity in the square brackets of the TD(0) update is a sort of error that measures the difference between the estimated value of $S_t$ and the better estimate given by $R_{t+1} + \gamma V(S_{t+1})$. This quantity is called \textbf{TD error} $\boldsymbol{\delta_t}$ and it represents the error in the estimate made at time $t$, as defined by the following:

\begin{equation}
    \delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
    \label{eq:ch5-tderrordeltat}
\end{equation}

Since $\delta_t$ depends on the next state and the next reward, the TD error at time $t$ will only be available at time $t+1$.

\section{Advantages and theoretical bases of TD methods}
Given what we have seen, we can already understand that Temporal Difference methods have advantages over both methods that require a model of the environment (such as Dynamic Programming) and Monte Carlo methods, as they cannot be applied in an online, fully incremental fashion like TD. This is often crucial because we may not have a complete model of the environment or the tasks that we are studying might not be divisible into episodes (such as the case of continuing tasks).

One may then ask if these methods still guarantee convergence to the correct values. Luckily, this is the case, as for any fixed policy $\pi$, TD(0) has been proven to converge to $v_\pi$ with probability 1 (under stochastic approximation conditions) if we decrease over time the value of the step size parameter $\alpha$, while only on average if $\alpha$ is statically selected to be sufficiently small.

\section{SARSA: on-policy Temporal Difference Control}
After seeing how to estimate the state-value function with TD(0) (the \textit{prediction problem}), we will now move on to the \textit{control problem} by introducing SARSA.

SARSA is an on-policy control method (it aims to estimate and improve the policy used to make decisions) and, as such, it requires us to estimate the \textbf{action-value function} $\boldsymbol{q_\pi (s,a)}$ for the current behavior policy\footnote{Since we are talking about on-policy methods, the policy also determines our behavior, thus ``behavior policy''.}  $\pi$ and for all the states $s$ and actions $a$, rather than the state-value function $v_\pi$ as we did in TD(0) (in short, we want to learn the values of the state-action pairs rather than the values of the single states). Formally, these cases are identical: they are both Markov chains with a reward process and the theorems that assured the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:

\begin{equation}
    Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \right]
    \label{eq:ch5-sarsaupdaterule}
\end{equation}

This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1},A_{t+1}) \doteq 0$. The name of this rule comes from the quintuple used to perform this update: $\left(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\right)$.

It is straightforward to design an on-policy control algorithm based on the SARSA prediction method. As in all on-policy methods, we continually estimate $q_\pi$ for the behavior policy $\pi$, and at the same time change $\pi$ towards greediness with respect to $q_\pi$. The \textbf{SARSA (on-policy TD control) algorithm for estimating $\boldsymbol{Q}$ values} is presented in the box below:

\begin{algorithm}[H]
\SetAlgoLined
\Parameters{step size $\alpha \in \left] 0,1 \right]$, small $\varepsilon > 0$}
\Initialize{
    \\
    \Indp
    $Q(s,a),\ \forall s \in \mathcal{S}^+, a \in \mathcal{A}(s)$, arbitrarily except that $Q(terminal, \cdot) = 0$
}

 \Loop{for each episode}{
    Initialize S\;
    Choose $A$ from $\mathcal{A}(S)$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy)\;
    \Loop{for each step of the episode}{
        Take action $A$, observe $R, S'$\;
        Choose $A'$ from $\mathcal{A}(S')$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy)\;
        $Q(S,A) \leftarrow Q(S,A) + \alpha \left[ R + \gamma Q(S',A') - Q(S,A) \right]$\;
        $S \leftarrow S'$\;
        $A \leftarrow A'$\;
    }{until $S$ is terminal}
 }
\caption{SARSA (on-policy TD control) for estimating $Q \approx q_*$}
\end{algorithm}

\section{Q-Learning: off-policy Temporal Difference Control}
On-policy methods like SARSA are not the only ones available to tackle the control problem. One of the early breakthroughs in reinforcement learning, in fact, was the development in 1989 of an off-policy Temporal Difference control algorithm known as \textbf{Q-Learning} and defined by:
\begin{equation}
    Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1},a) - Q(S_t,A_t) \right]
    \label{eq:ch5-qlearningupdaterule}
\end{equation}

As we can see, the update formula does not consider the action $A_{t+1}$ that we chose at time $t+1$, but a (possibly) different one: $a$, the one that has the highest $Q$ value. In this case, the \textbf{learned action-value function $\boldsymbol{Q}$ directly approximates the optimal action-value function $\boldsymbol{q_*}$ independent of the policy being followed}. The policy still has an effect in that it determines which state-action pairs are visited and updated; however, all that is required for correct convergence is that all pairs continue to be updated. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, $Q$ has been shown to converge with probability 1 to $q_*$. More information can be found in \cite{Watkins1992}, while an implementation is shown in the box below:

\begin{algorithm}[H]
\SetAlgoLined
\Parameters{step size $\alpha \in \left] 0,1 \right]$, small $\varepsilon > 0$}
\Initialize{
    \\
    \Indp
    $Q(s,a),\ \forall s \in \mathcal{S}^+, a \in \mathcal{A}(s)$, arbitrarily except that $Q(terminal, \cdot) = 0$
}

 \Loop{for each episode}{
    Initialize S\;
    \Loop{for each step of the episode}{
        Choose $A$ from $\mathcal{A}(S)$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy)\;
        Take action $A$, observe $R, S'$\;
        $Q(S,A) \leftarrow Q(S,A) + \alpha \left[ R + \gamma \max_a Q(S',a) - Q(S,A) \right]$\;
        $S \leftarrow S'$\;
    }{until $S$ is terminal}
 }
\caption{Q-Learning (off-policy TD control) for estimating $\pi \approx \pi_*$}
\end{algorithm}

\section{Summary}
The methods that we have seen in this chapter (SARSA and Q-Learning) are some of the most used ones in the reinforcement learning field, just like Decision Trees in Machine Learning and Min-Max in heuristics. They are often referred to as \textbf{tabular methods} since the state-action space must fit in a table in which every row corresponds to a state-action entry. This raises a question: what happens if we cannot fit all the entries in a table because they are too many? In that case we will need \textbf{function approximators rather than tables}: functions that, given in input the state (or the state and action), output the value function for the state (or the state and action). For these more complex cases, we will need \textbf{deep reinforcement learning}.