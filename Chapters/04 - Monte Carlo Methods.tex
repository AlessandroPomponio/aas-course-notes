\chapter{Monte Carlo Methods}
Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, we will focus on episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense.

Monte Carlo methods sample and average \textbf{returns} for each state-action pair much like the bandit methods we explored earlier sample and average \textbf{rewards} for each action. The main difference is that now there are multiple states, each acting like a different (but interrelated) bandit problem. That is, \textbf{the return after taking an action in one state depends on the actions taken in later states in the same episode} (it is the \textbf{average expected cumulative reward}). 

From this point on, we will assume that we do not have full knowledge of the underlying Markov Decision Process. We are then entering the \textbf{full reinforcement learning problem}, in which the underlying dynamics and characteristics of the system are unknown (e.g., robot exploration) or because the system is too complex (e.g., games). In these types of problems, we will have to \textbf{learn the value functions from sample returns}.

We will consider three problems:
\begin{enumerate}
    \item The \textbf{prediction problem}, where we want to estimate $v_\pi$ and $q_\pi$ for a fixed policy $\pi$. We are given a fixed policy (that is, we know how we play) and we try to estimate the expected cumulative reward.
    \item The \textbf{policy improvement problem}, where we still try to estimate $v_\pi$ and $q_\pi$, but this time we also try to improve the policy $\pi$. We try to estimate the value of each state and we use this information to play better.
    \item The \textbf{control problem}, where we try to estimate an optimal policy $\pi_*$ (e.g., we try to find the best way to play a game; the best way to build a datacenter to minimize energy consumption; â€¦).
\end{enumerate}

\section{Monte Carlo Prediction}
We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Before doing so, we must have clear in our minds that \textbf{the value of a state is the expected return (expected cumulative future discounted reward) starting from that state}. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.

More formally: we want to estimate $v_\pi (s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a \textbf{visit} to $s$. The same state may be visited multiple times in a certain episode: let us call the first time this happens the \textbf{first visit to} $\boldsymbol{s}$. The \textbf{first-visit Monte Carlo method} estimates $v_\pi (s)$ as the average of the returns following the \textit{``first visits''} to $s$, whereas the \textbf{every-visit Monte Carlo method} averages the returns following \textit{all visits} to $s$.

\subsection{First-visit Monte Carlo Prediction}
The algorithm for the \textit{first-visit Monte Carlo Prediction} is shown here:

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoVlined
\KwIn{a policy $\pi$ to be evaluated}
\Initialize{
    \\
    \Indp % add indent
    $V(s) \in \mathbb{R}$, arbitrarily, $\forall s \in \mathcal{S}$ \\
    $Returns(s) \leftarrow $ an empty list, $\forall s \in \mathcal{S}$ \\
    \Indm % remove indent
}

 \Loop{forever (for each episode)}{
    Generate an episode following $\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$\\
    $G \leftarrow 0$\;
    \Loop{for each step of the episode, $t = T-1, T-2, ..., 0$}{
        $G \leftarrow \gamma G + R_{t+1}$\;
        \Unless{$S_t$ appears in $S_0, S_1, ..., S_{t-1}$}{
            Append $G$ to $Returns(s)$\;
            $V(S_t) \leftarrow average(Returns(S_t))$\;
        }
    }
    
 }
 \caption{First-visit Monte Carlo Prediction}
\end{algorithm}

This means generating an episode where we play following policy $\pi$ (the set of probabilities associated to each action) and analyzing the ``trajectory'' that we followed during the game in a backwards manner. During this phase we will calculate the values for all the states that we traversed for the first time and the average cumulative reward we can expect from them.

\subsection{Every-visit (multi-visit) Monte Carlo Prediction}
If we remove from the \textit{first-visit Monte Carlo Prediction} algorithm the check on whether a state $S_t$ has already occurred, we obtain the \textbf{every-visit Monte Carlo Prediction}, which also converges to $v_\pi (s)$ as the number of visits to a certain state $s$ goes to infinity.

\section{Monte Carlo Estimation of Action Values}
The estimation of a state value makes sense when we have a model of the system: with it, in fact, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state. Without a model, however, it is necessary to estimate the value of each action in order for the value to be useful in suggesting a policy. 

The \textbf{policy evaluation problem} for action values is to estimate $q_\pi (s,a)$, the expected return when starting in state $s$, taking action $a$ and then following policy $\pi$. The methods for the Monte Carlo estimation of action values are essentially the same as those presented for state values, except that now we talk about \textbf{visits to the state-action pair} rather than to a state. A state-action pair $s,a$ is said to be visited in an episode if ever the state $s$ is visited and the action $a$ is taken in it. The first-visit Monte Carlo method averages the returns following the first time in each episode that the state was visited and the action was selected.

The only complication is that many state-action pairs may never be visited: if $\pi$ is a deterministic policy (to a certain state corresponds one and only one action), in fact, we would observe returns only for one of the actions of each state. With no returns to average, the Monte Carlo estimates of the other actions would subsequently not improve with experience. In order to compare alternatives, we then need to estimate the value of all the actions from each state, not only the one that is favored by our policy. This is the general problem of \textbf{maintaining exploration}.

For policy evaluation to work for action values, we must assure \textbf{continual exploration}. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes; we call this the assumption of \textbf{exploring starts}. This assumption is sometimes useful, but it cannot be relied upon in general (we may need to enumerate all the states, or they may not be valid): the most common alternative approach is to assure that all state-action pairs are encountered. This means \textbf{not following the current policy} (for example by using a stochastic policy).

\section{Monte Carlo Policy Improvement and Monte Carlo Control}
So far, we have assumed that the policy $\pi$ was fixed, like in the case of a statically defined set of probabilities for each action (the \textit{prediction problem}). However, by using methods typically known as \textbf{Monte Carlo Control}, we can improve the policy and reach the optimal one. In this section we will focus on \textbf{on-policy methods}, meaning \textbf{methods that attempt to evaluate or improve the policy that is used to make decisions}.

The overall idea is to proceed according to the \textbf{Generalized Policy Iteration (GPI)}, in which we maintain both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function. As we can imagine, these two changes work against each other to some extent, as each creates a ``moving target'' for the other, but together they cause both policy and value function to approach optimality.

With this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy $\pi_0$ and ending with the optimal policy and action-value function as such:

\begin{equation}
    \pi_0 \overset{E}{\rightarrow} q_{\pi_0} \overset{I}{\rightarrow} \pi_1 \overset{E}{\rightarrow} q_{\pi_1} \overset{I}{\rightarrow} \pi_2 \overset{E}{\rightarrow} ... \overset{I}{\rightarrow} \pi_* \overset{E}{\rightarrow} q_*
    \label{eq:ch3-generalizedpolicyiteration}
\end{equation}

Where $\overset{E}{\rightarrow}$ denotes a complete policy evaluation and $\overset{I}{\rightarrow}$ denotes a complete policy improvement. \textit{Policy evaluation} is performed as we mentioned in previous sections: many episodes are experienced, with the approximate action-value function approaching the true function asymptotically. \textit{Policy improvement} is done by making the policy greedy with respect to the current value function. Note how, since we have an action-value function, we do not need any model to construct the greedy policy: for any action-value function $q$, the corresponding greedy policy is the one that for each $s \in \mathcal{S}$, deterministically chooses an action with maximal action-value as such:

\begin{equation*}
    \pi(s) \doteq \argmax_a q(s,a)
\end{equation*}

Policy improvement can then be done by constructing each $\pi_{k+1}$ as the greedy policy with respect to $q_{\pi_k}$. The \textbf{policy improvement theorem} assures us that each $\pi_{k+1}$ is uniformly better than $\pi_k$ or just as good (in which case they are both optimal policies). This confirms that \textbf{the overall process converges to the optimal policy and optimal value function}.

\subsection{Monte Carlo Control algorithm with Exploring Starts}
Since we need to guarantee that all actions are selected ``infinitely often'', in some cases we can do so by means of \textbf{exploring starts}. A possible implementation of the \textbf{Monte Carlo Control algorithm with Exploring Starts} is shown in the box below:

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoVlined
\Initialize{
    \\
    \Indp % add indent
    $\pi(s) \in \mathcal{A}(s)$ (arbitrarily), $\forall s \in \mathcal{S}$ \\
    $Q(s,a) \in \mathbb{R}$ (arbitrarily), $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \\
    $Returns(s,a) \leftarrow $ an empty list, $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \\
    \Indm % remove indent
}

 \Loop{forever (for each episode)}{
    Choose a starting state and action $S_0 \in \mathcal{S}, A_0 \in \mathcal{A}(S_0)$ randomly, such that all pairs have probability $> 0$\;
    Generate an episode starting with $S_0,A_0$ and following $\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$\;
    $G \leftarrow 0$\;
    \Loop{for each step of the episode, $t = T-1, T-2, ..., 0$}{
        $G \leftarrow \gamma G + R_{t+1}$\;
        \Unless{the pair $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}$}{
            Append $G$ to $Returns(S_t, A_t)$\;
            $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))$\;
            $\pi(S_t) \leftarrow \argmax_a Q(S_t,a)$\;
        }
    }
    
 }
 \caption{Monte Carlo Control algorithm with Exploring Starts}
\end{algorithm}

With this algorithm, all the returns for each state-action pair are accumulated and averaged, irrespective of what policy was in force when they were observed. It is easy to see that it cannot converge to any suboptimal policy: if it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. \textbf{Stability is achieved only when both the policy and the value function are optimal}.

\subsection{Monte Carlo Control without Exploring Starts}
As we mentioned earlier on, the assumption of exploring starts is often unlikely to be valid; a possible alternative is to use \textbf{soft policies}, meaning that each action in each state has a nonzero probability of being chosen ($\pi(a \vert s) > 0, \forall s\in\mathcal{S},a\in\mathcal{A}(s)$), but gradually shifts closer and closer to a deterministic, optimal policy. The $\varepsilon$-greedy policy we have already seen previously is an example of $\varepsilon$-soft policy, in which all non-greedy actions have a (small) probability $\frac{\varepsilon}{\left\vert \mathcal{A}(s) \right\vert}$ of being selected.

Contrarily to what we did under the assumption of exploring starts, we cannot simply improve the policy by making it greedy with respect to the current value function, as it would prevent any exploration of the nongreedy actions. Fortunately, the Generalized Policy Iteration method does not require that the policy be taken all the way to a greedy policy, but only that it be moved \textit{towards} a greedy policy. That any $\varepsilon$-greedy policy with respect to $q_\pi$ is an improvement over any $\varepsilon$-soft policy $\pi$ is once again assured by the policy improvement theorem.

The \textbf{on-policy first-visit Monte Carlo control (for $\boldsymbol{\varepsilon}$-soft policies) algorithm} is presented in the box below:

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoVlined
\Parameters{small $\varepsilon > 0$}
\Initialize{
    \\
    \Indp % add indent
    $\pi \leftarrow$ an arbitrary $\varepsilon$-soft policy \\
    $Q(s,a) \in \mathbb{R}$ (arbitrarily), $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \\
    $Returns(s,a) \leftarrow $ an empty list, $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \\
    \Indm % remove indent
}

 \Loop{forever (for each episode)}{
    Generate an episode following $\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$\;
    $G \leftarrow 0$\;
    \Loop{for each step of the episode, $t = T-1, T-2, ..., 0$}{
        $G \leftarrow \gamma G + R_{t+1}$\;
        \Unless{the pair $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}$}{
            Append $G$ to $Returns(S_t, A_t)$\;
            $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))$\;
            $A^* \leftarrow \argmax_a Q(S_t,a) \quad$ (with ties broken arbitrarily)\;
            $\forall a \in \mathcal{A}(S_t)$:
            \begin{equation*}
                \pi(a \vert S_t) \leftarrow
                \begin{cases}
                            1 - \varepsilon + \frac{\varepsilon}{\left\vert \mathcal{A}(S_t) \right\vert} \quad & \text{if} \  a = A^* \\
                            \frac{\varepsilon}{\left\vert \mathcal{A}(S_t) \right\vert} \quad & \text{if} \  a \neq A^*
                \end{cases}
            \end{equation*}
        }
    }
    
 }
 \caption{On-policy first-visit MC control (for $\varepsilon$-soft policies)}
\end{algorithm}